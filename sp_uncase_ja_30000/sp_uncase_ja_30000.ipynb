{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/ikitaichi/.pyenv/versions/anaconda3-4.1.0/lib/python3.7/site-packages (0.1.81)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-27 02:31:42--  https://dumps.wikimedia.org/other/cirrussearch/20190422/jawiki-20190422-cirrussearch-content.json.gz\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 2620:0:861:4:208:80:155:106, 208.80.155.106\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|2620:0:861:4:208:80:155:106|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7333820534 (6.8G) [application/octet-stream]\n",
      "Saving to: ‘jawiki-20190422-cirrussearch-content.json.gz’\n",
      "\n",
      "jawiki-20190422-cir 100%[===================>]   6.83G  2.00MB/s    in 58m 43s \n",
      "\n",
      "2019-04-27 03:30:26 (1.99 MB/s) - ‘jawiki-20190422-cirrussearch-content.json.gz’ saved [7333820534/7333820534]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/other/cirrussearch/20190422/jawiki-20190422-cirrussearch-content.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(input_path, output_path, block_size=100000):\n",
    "    \"\"\"\n",
    "    text processes\n",
    "    1. lower\n",
    "    2. (split by newlines)\n",
    "    3. strip\n",
    "    \"\"\"\n",
    "    \n",
    "    def flush(text_lines):\n",
    "        with open(output_path, 'a') as f:\n",
    "            ret = f.write('\\n'.join(filter(\n",
    "                    lambda _:len(_)>0, \n",
    "                    (l.strip() for l in itertools.chain.from_iterable(text_lines))\n",
    "                )))\n",
    "            print('write:%s'%(ret))\n",
    "        text_lines.clear()\n",
    "\n",
    "    with gzip.open(input_path) as f:\n",
    "        text_lines = []\n",
    "        for line in f:\n",
    "            json_line = json.loads(line)\n",
    "            if \"text\" in json_line:\n",
    "                text = json_line[\"text\"]\n",
    "                text_lines.append(text.lower().split('\\n'))\n",
    "                if len(text_lines) > block_size:\n",
    "                    flush(text_lines)\n",
    "        if len(text_lines) > 0:\n",
    "            flush(text_lines)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write:206537272\n",
      "write:183202756\n",
      "write:186570109\n",
      "write:181526538\n",
      "write:198664898\n",
      "write:192250534\n",
      "write:234313888\n",
      "write:225161454\n",
      "write:229207931\n",
      "write:212864148\n",
      "write:223760169\n",
      "write:113536913\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "extract_text('jawiki-20190422-cirrussearch-content.json.gz', 'jawiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -n800000 jawiki.txt > jawiki_800000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as sp\n",
    "\n",
    "vocab_size=30000\n",
    "vocab_reserved=16\n",
    "vocab_reserved_used=3 # account for pad, unk, space\n",
    "vocab_reserved_unused=vocab_reserved - vocab_reserved_used\n",
    "model_prefix='sp_uncase_ja_%d'%(vocab_size)\n",
    "_input='jawiki_800000.txt'\n",
    "\n",
    "command = ' '.join((\n",
    "    '--pad_id=0',\n",
    "    '--unk_id=1',\n",
    "    '--bos_id=-1', \n",
    "    '--eos_id=-1',\n",
    "    '--add_dummy_prefix=False',\n",
    "    '--user_defined_symbols=\\u2581',\n",
    "    '--input_sentence_size=500000',\n",
    "    '--vocab_size=%d'%(vocab_size-vocab_reserved_unused), \n",
    "    '--model_prefix=%s'%(model_prefix),\n",
    "    '--input=%s'%(_input),\n",
    "))\n",
    "# train ran on python interpreter\n",
    "# sp.SentencePieceTrainer.Train(command)\n",
    "\"\"\"\n",
    "sentencepiece_trainer.cc(116) LOG(INFO) Running command: --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=-1 --add_dummy_prefix=False --user_defined_symbols=▁ --input_sentence_size=500000 --vocab_size=29987 --model_prefix=sp_uncase_ja_30000 --input=jawiki_800000.txt\n",
    "sentencepiece_trainer.cc(49) LOG(INFO) Starts training with :\n",
    "TrainerSpec {\n",
    "  input: jawiki_800000.txt\n",
    "  input_format:\n",
    "  model_prefix: sp_uncase_ja_30000\n",
    "  model_type: UNIGRAM\n",
    "  vocab_size: 29987\n",
    "  self_test_sample_size: 0\n",
    "  character_coverage: 0.9995\n",
    "  input_sentence_size: 500000\n",
    "  shuffle_input_sentence: 1\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  max_sentence_length: 4192\n",
    "  num_threads: 16\n",
    "  num_sub_iterations: 2\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: 1\n",
    "  split_by_number: 1\n",
    "  split_by_whitespace: 1\n",
    "  treat_whitespace_as_suffix: 0\n",
    "  user_defined_symbols: ▁\n",
    "  hard_vocab_limit: 1\n",
    "  use_all_vocab: 0\n",
    "  unk_id: 1\n",
    "  bos_id: -1\n",
    "  eos_id: -1\n",
    "  pad_id: 0\n",
    "  unk_piece: <unk>\n",
    "  bos_piece: <s>\n",
    "  eos_piece: </s>\n",
    "  pad_piece: <pad>\n",
    "  unk_surface:  ⁇\n",
    "}\n",
    "NormalizerSpec {\n",
    "  name: nmt_nfkc\n",
    "  add_dummy_prefix: 0\n",
    "  remove_extra_whitespaces: 1\n",
    "  escape_whitespaces: 1\n",
    "  normalization_rule_tsv:\n",
    "}\n",
    "\n",
    "trainer_interface.cc(267) LOG(INFO) Loading corpus: jawiki_800000.txt\n",
    "trainer_interface.cc(287) LOG(WARNING) Found too long line (5029 > 4192).\n",
    "trainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\n",
    "trainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
    "trainer_interface.cc(317) LOG(INFO) Sampled 500000 sentences from 566490 sentences.\n",
    "trainer_interface.cc(321) LOG(INFO) Skipped 233510 too long sentences.\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <pad>\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: ▁\n",
    "trainer_interface.cc(335) LOG(INFO) Normalizing sentences...\n",
    "trainer_interface.cc(384) LOG(INFO) all chars count=356559894\n",
    "trainer_interface.cc(392) LOG(INFO) Done: 99.95% characters are covered.\n",
    "trainer_interface.cc(402) LOG(INFO) Alphabet size=4792\n",
    "trainer_interface.cc(403) LOG(INFO) Final character coverage=0.9995\n",
    "trainer_interface.cc(435) LOG(INFO) Done! preprocessed 500000 sentences.\n",
    "unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...\n",
    "unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...\n",
    "unigram_model_trainer.cc(184) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
    "trainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 500000\n",
    "trainer_interface.cc(451) LOG(INFO) Done! 499927\n",
    "unigram_model_trainer.cc(470) LOG(INFO) Using 499927 sentences for EM training\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=754660 obj=3846.39 num_tokens=150019983 num_tokens/piece=198.791\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=634831 obj=3751.48 num_tokens=152882454 num_tokens/piece=240.824\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=476004 obj=3751.04 num_tokens=154629416 num_tokens/piece=324.849\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=475258 obj=3747.82 num_tokens=154974599 num_tokens/piece=326.085\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=356442 obj=3693.68 num_tokens=155843177 num_tokens/piece=437.219\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=356430 obj=3727.13 num_tokens=155941246 num_tokens/piece=437.509\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=267319 obj=3705.3 num_tokens=157876067 num_tokens/piece=590.591\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=267318 obj=3758.53 num_tokens=157895542 num_tokens/piece=590.666\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=200487 obj=3726.27 num_tokens=160790735 num_tokens/piece=802.001\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=200487 obj=3718.87 num_tokens=160796597 num_tokens/piece=802.03\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=150365 obj=3756.64 num_tokens=164323475 num_tokens/piece=1092.83\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=150365 obj=3748.22 num_tokens=164331323 num_tokens/piece=1092.88\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=112773 obj=3794.45 num_tokens=168342289 num_tokens/piece=1492.75\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=112773 obj=3785.19 num_tokens=168347417 num_tokens/piece=1492.8\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=84579 obj=3838.9 num_tokens=172792702 num_tokens/piece=2042.97\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=84579 obj=3828.82 num_tokens=172805805 num_tokens/piece=2043.13\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=63434 obj=3887.71 num_tokens=177664984 num_tokens/piece=2800.78\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=63434 obj=3876.88 num_tokens=177668862 num_tokens/piece=2800.85\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=47575 obj=3942.01 num_tokens=183113762 num_tokens/piece=3848.95\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=47575 obj=3930.28 num_tokens=183112525 num_tokens/piece=3848.92\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=35681 obj=4001.54 num_tokens=189085471 num_tokens/piece=5299.33\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=35681 obj=3988.38 num_tokens=189084926 num_tokens/piece=5299.32\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=32985 obj=4007.85 num_tokens=190813142 num_tokens/piece=5784.85\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=32985 obj=4004.1 num_tokens=190819107 num_tokens/piece=5785.03\n",
    "trainer_interface.cc(507) LOG(INFO) Saving model: sp_uncase_ja_30000.model\n",
    "trainer_interface.cc(531) LOG(INFO) Saving vocabs: sp_uncase_ja_30000.vocab\n",
    "\"\"\"\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification of .vocab file to append reserved place holders\n",
    "with open(model_prefix + '.vocab') as f:\n",
    "    lines = [_ for _ in f.read().split('\\n') if len(_) > 0]\n",
    "\n",
    "import shutil\n",
    "shutil.copy(model_prefix + '.vocab', model_prefix + '.old.vocab')\n",
    "    \n",
    "lines_reserved = lines[:vocab_reserved_used]\n",
    "lines_unused = ['unused_%d\\t0'%i for i in range(0, vocab_reserved_unused)]\n",
    "lines_normal = lines[vocab_reserved_used:]\n",
    "lines = lines_reserved + lines_unused + lines_normal\n",
    "assert len(lines) == vocab_size, 'len(lines)=%d vs vocab_size=%d'%(len(lines), vocab_size)\n",
    "\n",
    "with open(model_prefix + '.vocab', 'w') as f:\n",
    "    f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
