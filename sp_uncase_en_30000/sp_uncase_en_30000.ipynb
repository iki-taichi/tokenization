{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/ikitaichi/.pyenv/versions/anaconda3-4.1.0/lib/python3.7/site-packages (0.1.81)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-27 04:09:59--  https://dumps.wikimedia.org/other/cirrussearch/20190422/enwiki-20190422-cirrussearch-content.json.gz\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 2620:0:861:4:208:80:155:106, 208.80.155.106\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|2620:0:861:4:208:80:155:106|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28155129067 (26G) [application/octet-stream]\n",
      "Saving to: ‘enwiki-20190422-cirrussearch-content.json.gz’\n",
      "\n",
      "enwiki-20190422-cir 100%[===================>]  26.22G  1.97MB/s    in 3h 49m  \n",
      "\n",
      "2019-04-27 07:59:13 (1.95 MB/s) - ‘enwiki-20190422-cirrussearch-content.json.gz’ saved [28155129067/28155129067]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/other/cirrussearch/20190422/enwiki-20190422-cirrussearch-content.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(input_path, output_path, block_size=100000):\n",
    "    \"\"\"\n",
    "    text processes\n",
    "    1. lower\n",
    "    2. (split by newlines)\n",
    "    3. strip\n",
    "    \"\"\"\n",
    "    \n",
    "    def flush(text_lines):\n",
    "        with open(output_path, 'a') as f:\n",
    "            ret = f.write('\\n'.join(filter(\n",
    "                    lambda _:len(_)>0, \n",
    "                    (l.strip() for l in itertools.chain.from_iterable(text_lines))\n",
    "                )))\n",
    "            print('write:%s'%(ret))\n",
    "        text_lines.clear()\n",
    "\n",
    "    with gzip.open(input_path) as f:\n",
    "        text_lines = []\n",
    "        for line in f:\n",
    "            json_line = json.loads(line)\n",
    "            if \"text\" in json_line:\n",
    "                text = json_line[\"text\"]\n",
    "                text_lines.append(text.lower().split('\\n'))\n",
    "                if len(text_lines) > block_size:\n",
    "                    flush(text_lines)\n",
    "        if len(text_lines) > 0:\n",
    "            flush(text_lines)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write:369702687\n",
      "write:358255001\n",
      "write:339906391\n",
      "write:329798261\n",
      "write:317134541\n",
      "write:325287199\n",
      "write:314865612\n",
      "write:303227519\n",
      "write:313747929\n",
      "write:297361084\n",
      "write:271422697\n",
      "write:262897437\n",
      "write:289770085\n",
      "write:302961892\n",
      "write:279494464\n",
      "write:245401108\n",
      "write:234794586\n",
      "write:232699464\n",
      "write:260179880\n",
      "write:268285622\n",
      "write:243880294\n",
      "write:283496239\n",
      "write:230539175\n",
      "write:232695503\n",
      "write:318426960\n",
      "write:307303468\n",
      "write:253428083\n",
      "write:264312258\n",
      "write:338572474\n",
      "write:336930229\n",
      "write:289385204\n",
      "write:228614831\n",
      "write:260443451\n",
      "write:230449676\n",
      "write:233468237\n",
      "write:218666297\n",
      "write:222052512\n",
      "write:217431745\n",
      "write:220272896\n",
      "write:218042917\n",
      "write:235347744\n",
      "write:331820823\n",
      "write:334924592\n",
      "write:358884553\n",
      "write:465671752\n",
      "write:544773966\n",
      "write:521944186\n",
      "write:598539592\n",
      "write:513487035\n",
      "write:652594881\n",
      "write:621448485\n",
      "write:590982259\n",
      "write:575114906\n",
      "write:490271620\n",
      "write:391809125\n",
      "write:536767348\n",
      "write:760846210\n",
      "write:786657312\n",
      "write:350909358\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "extract_text('enwiki-20190422-cirrussearch-content.json.gz', 'enwiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -n800000 enwiki.txt > enwiki_800000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as sp\n",
    "\n",
    "vocab_size=30000\n",
    "vocab_reserved=16\n",
    "vocab_reserved_used=2 # account for pad, unk\n",
    "vocab_reserved_unused=vocab_reserved - vocab_reserved_used\n",
    "model_prefix='sp_uncase_en_%d'%(vocab_size)\n",
    "_input='enwiki_800000.txt'\n",
    "\n",
    "command = ' '.join((\n",
    "    '--pad_id=0',\n",
    "    '--unk_id=1',\n",
    "    '--bos_id=-1', \n",
    "    '--eos_id=-1',\n",
    "    '--add_dummy_prefix=True',\n",
    "    '--input_sentence_size=500000',\n",
    "    '--max_sentence_length=4192',\n",
    "    '--vocab_size=%d'%(vocab_size-vocab_reserved_unused), \n",
    "    '--model_prefix=%s'%(model_prefix),\n",
    "    '--input=%s'%(_input),\n",
    "))\n",
    "#sp.SentencePieceTrainer.Train(command)\n",
    "\n",
    "# Train method in this cell ran in a local shell to show logs.\n",
    "\"\"\"\n",
    "sentencepiece_trainer.cc(116) LOG(INFO) Running command: --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=-1 --add_dummy_prefix=True --input_sentence_size=500000 --max_sentence_length=4192 --vocab_size=29986 --model_prefix=sp_uncase_en_30000 --input=enwiki_800000.txt\n",
    "sentencepiece_trainer.cc(49) LOG(INFO) Starts training with :\n",
    "TrainerSpec {\n",
    "  input: enwiki_800000.txt\n",
    "  input_format:\n",
    "  model_prefix: sp_uncase_en_30000\n",
    "  model_type: UNIGRAM\n",
    "  vocab_size: 29986\n",
    "  self_test_sample_size: 0\n",
    "  character_coverage: 0.9995\n",
    "  input_sentence_size: 500000\n",
    "  shuffle_input_sentence: 1\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  max_sentence_length: 4192\n",
    "  num_threads: 16\n",
    "  num_sub_iterations: 2\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: 1\n",
    "  split_by_number: 1\n",
    "  split_by_whitespace: 1\n",
    "  treat_whitespace_as_suffix: 0\n",
    "  hard_vocab_limit: 1\n",
    "  use_all_vocab: 0\n",
    "  unk_id: 1\n",
    "  bos_id: -1\n",
    "  eos_id: -1\n",
    "  pad_id: 0\n",
    "  unk_piece: <unk>\n",
    "  bos_piece: <s>\n",
    "  eos_piece: </s>\n",
    "  pad_piece: <pad>\n",
    "  unk_surface:  ⁇\n",
    "}\n",
    "NormalizerSpec {\n",
    "  name: nmt_nfkc\n",
    "  add_dummy_prefix: 1\n",
    "  remove_extra_whitespaces: 1\n",
    "  escape_whitespaces: 1\n",
    "  normalization_rule_tsv:\n",
    "}\n",
    "\n",
    "trainer_interface.cc(267) LOG(INFO) Loading corpus: enwiki_800000.txt\n",
    "trainer_interface.cc(287) LOG(WARNING) Found too long line (8115 > 4192).\n",
    "trainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\n",
    "trainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
    "trainer_interface.cc(317) LOG(INFO) Sampled 500000 sentences from 625649 sentences.\n",
    "trainer_interface.cc(321) LOG(INFO) Skipped 174351 too long sentences.\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <pad>\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\n",
    "trainer_interface.cc(335) LOG(INFO) Normalizing sentences...\n",
    "trainer_interface.cc(384) LOG(INFO) all chars count=661312249\n",
    "trainer_interface.cc(392) LOG(INFO) Done: 99.95% characters are covered.\n",
    "trainer_interface.cc(402) LOG(INFO) Alphabet size=691\n",
    "trainer_interface.cc(403) LOG(INFO) Final character coverage=0.9995\n",
    "trainer_interface.cc(435) LOG(INFO) Done! preprocessed 500000 sentences.\n",
    "unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...\n",
    "unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...\n",
    "unigram_model_trainer.cc(184) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
    "trainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 500000\n",
    "trainer_interface.cc(451) LOG(INFO) Done! 4468864\n",
    "unigram_model_trainer.cc(470) LOG(INFO) Using 4468864 sentences for EM training\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=656359 obj=13.1717 num_tokens=14744373 num_tokens/piece=22.4639\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=552683 obj=10.726 num_tokens=14702348 num_tokens/piece=26.6018\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=414484 obj=10.6869 num_tokens=14813221 num_tokens/piece=35.7389\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=414289 obj=10.6779 num_tokens=14831138 num_tokens/piece=35.799\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=310716 obj=10.6906 num_tokens=15115184 num_tokens/piece=48.6463\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=310712 obj=10.6859 num_tokens=15118548 num_tokens/piece=48.6578\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=233034 obj=10.7225 num_tokens=15530424 num_tokens/piece=66.6445\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=233032 obj=10.7176 num_tokens=15530594 num_tokens/piece=66.6458\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=174774 obj=10.7758 num_tokens=16018682 num_tokens/piece=91.6537\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=174774 obj=10.7568 num_tokens=16017777 num_tokens/piece=91.6485\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=131080 obj=10.8469 num_tokens=16489249 num_tokens/piece=125.795\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=131079 obj=10.8303 num_tokens=16487205 num_tokens/piece=125.781\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=98309 obj=10.9328 num_tokens=16969299 num_tokens/piece=172.612\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=98309 obj=10.9103 num_tokens=16968886 num_tokens/piece=172.608\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=73731 obj=11.0365 num_tokens=17483639 num_tokens/piece=237.127\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=73731 obj=11.0113 num_tokens=17483325 num_tokens/piece=237.123\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=55298 obj=11.1549 num_tokens=18057591 num_tokens/piece=326.551\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=55298 obj=11.1257 num_tokens=18059618 num_tokens/piece=326.587\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=41467 obj=11.2863 num_tokens=18667518 num_tokens/piece=450.178\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=41451 obj=11.2634 num_tokens=18667016 num_tokens/piece=450.339\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=32984 obj=11.3992 num_tokens=19193520 num_tokens/piece=581.904\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=32984 obj=11.3677 num_tokens=19197473 num_tokens/piece=582.024\n",
    "trainer_interface.cc(507) LOG(INFO) Saving model: sp_uncase_en_30000.model\n",
    "trainer_interface.cc(531) LOG(INFO) Saving vocabs: sp_uncase_en_30000.vocab\n",
    "True\n",
    "\"\"\"\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification of .vocab file to append reserved place holders\n",
    "with open(model_prefix + '.vocab') as f:\n",
    "    lines = [_ for _ in f.read().split('\\n') if len(_) > 0]\n",
    "\n",
    "import shutil\n",
    "shutil.copy(model_prefix + '.vocab', model_prefix + '.old.vocab')\n",
    "    \n",
    "lines_reserved = lines[:vocab_reserved_used]\n",
    "lines_unused = ['unused_%d\\t0'%i for i in range(0, vocab_reserved_unused)]\n",
    "lines_normal = lines[vocab_reserved_used:]\n",
    "lines = lines_reserved + lines_unused + lines_normal\n",
    "assert len(lines) == vocab_size, 'len(lines)=%d vs vocab_size=%d'%(len(lines), vocab_size)\n",
    "\n",
    "with open(model_prefix + '.vocab', 'w') as f:\n",
    "    f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
