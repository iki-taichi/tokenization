{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/ikitaichi/.pyenv/versions/anaconda3-4.1.0/lib/python3.7/site-packages (0.1.81)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-27 02:31:42--  https://dumps.wikimedia.org/other/cirrussearch/20190422/jawiki-20190422-cirrussearch-content.json.gz\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 2620:0:861:4:208:80:155:106, 208.80.155.106\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|2620:0:861:4:208:80:155:106|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7333820534 (6.8G) [application/octet-stream]\n",
      "Saving to: ‘jawiki-20190422-cirrussearch-content.json.gz’\n",
      "\n",
      "jawiki-20190422-cir 100%[===================>]   6.83G  2.00MB/s    in 58m 43s \n",
      "\n",
      "2019-04-27 03:30:26 (1.99 MB/s) - ‘jawiki-20190422-cirrussearch-content.json.gz’ saved [7333820534/7333820534]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/other/cirrussearch/20190422/jawiki-20190422-cirrussearch-content.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-27 04:09:59--  https://dumps.wikimedia.org/other/cirrussearch/20190422/enwiki-20190422-cirrussearch-content.json.gz\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 2620:0:861:4:208:80:155:106, 208.80.155.106\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|2620:0:861:4:208:80:155:106|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28155129067 (26G) [application/octet-stream]\n",
      "Saving to: ‘enwiki-20190422-cirrussearch-content.json.gz’\n",
      "\n",
      "enwiki-20190422-cir 100%[===================>]  26.22G  1.97MB/s    in 3h 49m  \n",
      "\n",
      "2019-04-27 07:59:13 (1.95 MB/s) - ‘enwiki-20190422-cirrussearch-content.json.gz’ saved [28155129067/28155129067]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/other/cirrussearch/20190422/enwiki-20190422-cirrussearch-content.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(input_path, output_path, block_size=100000):\n",
    "    \"\"\"\n",
    "    text processes\n",
    "    1. lower\n",
    "    2. (split by newlines)\n",
    "    3. strip\n",
    "    \"\"\"\n",
    "    \n",
    "    def flush(text_lines):\n",
    "        with open(output_path, 'a') as f:\n",
    "            ret = f.write('\\n'.join(filter(\n",
    "                    lambda _:len(_)>0, \n",
    "                    (l.strip() for l in itertools.chain.from_iterable(text_lines))\n",
    "                )))\n",
    "            print('write:%s'%(ret))\n",
    "        text_lines.clear()\n",
    "\n",
    "    with gzip.open(input_path) as f:\n",
    "        text_lines = []\n",
    "        for line in f:\n",
    "            json_line = json.loads(line)\n",
    "            if \"text\" in json_line:\n",
    "                text = json_line[\"text\"]\n",
    "                text_lines.append(text.lower().split('\\n'))\n",
    "                if len(text_lines) > block_size:\n",
    "                    flush(text_lines)\n",
    "        if len(text_lines) > 0:\n",
    "            flush(text_lines)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write:369702687\n",
      "write:358255001\n",
      "write:339906391\n",
      "write:329798261\n",
      "write:317134541\n",
      "write:325287199\n",
      "write:314865612\n",
      "write:303227519\n",
      "write:313747929\n",
      "write:297361084\n",
      "write:271422697\n",
      "write:262897437\n",
      "write:289770085\n",
      "write:302961892\n",
      "write:279494464\n",
      "write:245401108\n",
      "write:234794586\n",
      "write:232699464\n",
      "write:260179880\n",
      "write:268285622\n",
      "write:243880294\n",
      "write:283496239\n",
      "write:230539175\n",
      "write:232695503\n",
      "write:318426960\n",
      "write:307303468\n",
      "write:253428083\n",
      "write:264312258\n",
      "write:338572474\n",
      "write:336930229\n",
      "write:289385204\n",
      "write:228614831\n",
      "write:260443451\n",
      "write:230449676\n",
      "write:233468237\n",
      "write:218666297\n",
      "write:222052512\n",
      "write:217431745\n",
      "write:220272896\n",
      "write:218042917\n",
      "write:235347744\n",
      "write:331820823\n",
      "write:334924592\n",
      "write:358884553\n",
      "write:465671752\n",
      "write:544773966\n",
      "write:521944186\n",
      "write:598539592\n",
      "write:513487035\n",
      "write:652594881\n",
      "write:621448485\n",
      "write:590982259\n",
      "write:575114906\n",
      "write:490271620\n",
      "write:391809125\n",
      "write:536767348\n",
      "write:760846210\n",
      "write:786657312\n",
      "write:350909358\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "extract_text('enwiki-20190422-cirrussearch-content.json.gz', 'enwiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write:206537272\n",
      "write:183202756\n",
      "write:186570109\n",
      "write:181526538\n",
      "write:198664898\n",
      "write:192250534\n",
      "write:234313888\n",
      "write:225161454\n",
      "write:229207931\n",
      "write:212864148\n",
      "write:223760169\n",
      "write:113536913\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "extract_text('jawiki-20190422-cirrussearch-content.json.gz', 'jawiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll use 600,000 inputs because of memory capacity.\n",
    "# preparing 600,000 sentences in Japanese and English respectively \n",
    "# in preparation for omitting too long sentences.\n",
    "!shuf -n600000 enwiki.txt > enwiki_600000.txt\n",
    "!shuf -n600000 jawiki.txt > jawiki_600000.txt\n",
    "!cat enwiki_600000.txt jawiki_600000.txt > inputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as sp\n",
    "\n",
    "vocab_size=40000\n",
    "vocab_reserved=16\n",
    "vocab_reserved_used=3 # account for pad, unk, space\n",
    "vocab_reserved_unused=vocab_reserved - vocab_reserved_used\n",
    "model_prefix='sp_uncase_en_ja_%d'%(vocab_size)\n",
    "_input='inputs.txt'\n",
    "\n",
    "command = ' '.join((\n",
    "    '--pad_id=0',\n",
    "    '--unk_id=1',\n",
    "    '--bos_id=-1', \n",
    "    '--eos_id=-1',\n",
    "    '--add_dummy_prefix=False',\n",
    "    '--user_defined_symbols=\\u2581',\n",
    "    '--input_sentence_size=600000',\n",
    "    '--vocab_size=%d'%(vocab_size-vocab_reserved_unused), \n",
    "    '--model_prefix=%s'%(model_prefix),\n",
    "    '--input=%s'%(_input),\n",
    "))\n",
    "#sp.SentencePieceTrainer.Train(command)\n",
    "\n",
    "# Train method in this cell ran in a local shell to show logs.\n",
    "\"\"\"\n",
    "sentencepiece_trainer.cc(116) LOG(INFO) Running command: --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=-1 --add_dummy_prefix=False --user_defined_symbols=? --input_sentence_size=600000 --vocab_size=39987 --model_prefix=sp_uncase_en_ja_40000 --input=inputs.txt\n",
    "sentencepiece_trainer.cc(49) LOG(INFO) Starts training with :\n",
    "TrainerSpec {\n",
    "  input: inputs.txt\n",
    "  input_format:\n",
    "  model_prefix: sp_uncase_en_ja_40000\n",
    "  model_type: UNIGRAM\n",
    "  vocab_size: 39987\n",
    "  self_test_sample_size: 0\n",
    "  character_coverage: 0.9995\n",
    "  input_sentence_size: 600000\n",
    "  shuffle_input_sentence: 1\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  max_sentence_length: 4192\n",
    "  num_threads: 16\n",
    "  num_sub_iterations: 2\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: 1\n",
    "  split_by_number: 1\n",
    "  split_by_whitespace: 1\n",
    "  treat_whitespace_as_suffix: 0\n",
    "  user_defined_symbols: ?\n",
    "  hard_vocab_limit: 1\n",
    "  use_all_vocab: 0\n",
    "  unk_id: 1\n",
    "  bos_id: -1\n",
    "  eos_id: -1\n",
    "  pad_id: 0\n",
    "  unk_piece: <unk>\n",
    "  bos_piece: <s>\n",
    "  eos_piece: </s>\n",
    "  pad_piece: <pad>\n",
    "  unk_surface:  ?\n",
    "}\n",
    "NormalizerSpec {\n",
    "  name: nmt_nfkc\n",
    "  add_dummy_prefix: 0\n",
    "  remove_extra_whitespaces: 1\n",
    "  escape_whitespaces: 1\n",
    "  normalization_rule_tsv:\n",
    "}\n",
    "\n",
    "trainer_interface.cc(267) LOG(INFO) Loading corpus: inputs.txt\n",
    "trainer_interface.cc(287) LOG(WARNING) Found too long line (14095 > 4192).\n",
    "trainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\n",
    "trainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
    "trainer_interface.cc(317) LOG(INFO) Sampled 600000 sentences from 893819 sentences.\n",
    "trainer_interface.cc(321) LOG(INFO) Skipped 306181 too long sentences.\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <pad>\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\n",
    "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: ?\n",
    "trainer_interface.cc(335) LOG(INFO) Normalizing sentences...\n",
    "trainer_interface.cc(384) LOG(INFO) all chars count=620184844\n",
    "trainer_interface.cc(392) LOG(INFO) Done: 99.95% characters are covered.\n",
    "trainer_interface.cc(402) LOG(INFO) Alphabet size=3843\n",
    "trainer_interface.cc(403) LOG(INFO) Final character coverage=0.9995\n",
    "trainer_interface.cc(435) LOG(INFO) Done! preprocessed 600000 sentences.\n",
    "unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...\n",
    "unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...\n",
    "unigram_model_trainer.cc(184) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
    "trainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 600000\n",
    "trainer_interface.cc(451) LOG(INFO) Done! 599929\n",
    "unigram_model_trainer.cc(470) LOG(INFO) Using 599929 sentences for EM training\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=739264 obj=6257.83 num_tokens=237723486 num_tokens/piece=321.568\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=620449 obj=6522.22 num_tokens=239478461 num_tokens/piece=385.976\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=465317 obj=6504.44 num_tokens=240315495 num_tokens/piece=516.455\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=465197 obj=6516.07 num_tokens=240666115 num_tokens/piece=517.342\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=348896 obj=6349.2 num_tokens=241613356 num_tokens/piece=692.508\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=348892 obj=6517.09 num_tokens=241672250 num_tokens/piece=692.685\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=261668 obj=6354.92 num_tokens=243677636 num_tokens/piece=931.247\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=261666 obj=6359.98 num_tokens=243686607 num_tokens/piece=931.289\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=196248 obj=6380.03 num_tokens=246591897 num_tokens/piece=1256.53\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=196248 obj=6374.55 num_tokens=246589702 num_tokens/piece=1256.52\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=147186 obj=6401.31 num_tokens=250098677 num_tokens/piece=1699.2\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=147186 obj=6398.36 num_tokens=250103244 num_tokens/piece=1699.23\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=110389 obj=6434.43 num_tokens=254137323 num_tokens/piece=2302.2\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=110389 obj=6435.06 num_tokens=254149810 num_tokens/piece=2302.31\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=82791 obj=6479 num_tokens=258699951 num_tokens/piece=3124.74\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=82791 obj=6470.84 num_tokens=258698437 num_tokens/piece=3124.72\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=62093 obj=6517.19 num_tokens=263779515 num_tokens/piece=4248.14\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=62093 obj=6510.74 num_tokens=263786381 num_tokens/piece=4248.25\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=46569 obj=6564.45 num_tokens=269611399 num_tokens/piece=5789.5\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=46569 obj=6554.53 num_tokens=269610046 num_tokens/piece=5789.47\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=43985 obj=6570.5 num_tokens=270864362 num_tokens/piece=6158.11\n",
    "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=43985 obj=6564.9 num_tokens=270866642 num_tokens/piece=6158.16\n",
    "trainer_interface.cc(507) LOG(INFO) Saving model: sp_uncase_en_ja_40000.model\n",
    "trainer_interface.cc(531) LOG(INFO) Saving vocabs: sp_uncase_en_ja_40000.vocab\n",
    "True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification of .vocab file to append reserved place holders\n",
    "with open(model_prefix + '.vocab') as f:\n",
    "    lines = [_ for _ in f.read().split('\\n') if len(_) > 0]\n",
    "\n",
    "import shutil\n",
    "shutil.copy(model_prefix + '.vocab', model_prefix + '.old.vocab')\n",
    "    \n",
    "lines_reserved = lines[:vocab_reserved_used]\n",
    "lines_unused = ['unused_%d\\t0'%i for i in range(0, vocab_reserved_unused)]\n",
    "lines_normal = lines[vocab_reserved_used:]\n",
    "lines = lines_reserved + lines_unused + lines_normal\n",
    "assert len(lines) == vocab_size, 'len(lines)=%d vs vocab_size=%d'%(len(lines), vocab_size)\n",
    "\n",
    "with open(model_prefix + '.vocab', 'w') as f:\n",
    "    f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
